## ğŸ‰ Introduction

Welcome to PILOT, a pre-trained model-based continual learning toolbox <a href="https://arxiv.org/abs/2309.07117">[Paper]</a>. On the one hand, PILOT implements some state-of-the-art class-incremental learning algorithms based on pre-trained models, such as L2P, DualPrompt, and CODA-Prompt. On the other hand, PILOT also fits typical class-incremental learning algorithmswithin the context of pre-trained models to evaluate their effectiveness.

æ¬¢è¿æ¥åˆ° PILOTï¼Œä¸€ä¸ªåŸºäºé¢„è®­ç»ƒæ¨¡å‹çš„æŒç»­å­¦ä¹ å·¥å…·ç®±ï¼ŒPILOT å®ç°äº†ä¸€äº›åŸºäºé¢„è®­ç»ƒæ¨¡å‹çš„æœ€å…ˆè¿›çš„ç±»å¢é‡å­¦ä¹ ç®—æ³•ï¼Œå¦‚ L2Pã€DualPrompt å’Œ CODA-Promptã€‚å¦ä¸€æ–¹é¢ï¼ŒPILOT è¿˜å°†å…¸å‹çš„ç±»å¢é‡å­¦ä¹ ç®—æ³• (_e.g._, FOSTER, and MEMO)æ‹Ÿåˆåœ¨é¢„è®­ç»ƒæ¨¡å‹çš„ä¸Šä¸‹æ–‡ä¸­ï¼Œä»¥è¯„ä¼°å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸŒŸ Methods Reproduced

- `FineTune`: Baseline method which simply updates parameters on new tasks.
- `iCaRL`: iCaRL: Incremental Classifier and Representation Learning. CVPR 2017 [[paper](https://arxiv.org/abs/1611.07725)]
- `Coil`: Co-Transport for Class-Incremental Learning. ACMMM 2021 [[paper](https://arxiv.org/abs/2107.12654)]
- `DER`: DER: Dynamically Expandable Representation for Class Incremental Learning. CVPR 2021 [[paper](https://arxiv.org/abs/2103.16788)]
- `FOSTER`: Feature Boosting and Compression for Class-incremental Learning. ECCV 2022 [[paper](https://arxiv.org/abs/2204.04662)]
- `MEMO`: A Model or 603 Exemplars: Towards Memory-Efficient Class-Incremental Learning. ICLR 2023 Spotlight [[paper](https://openreview.net/forum?id=S07feAlQHgM)]
- `L2P`: Learning to Prompt for Continual Learning. CVPR 2022 [[paper](https://arxiv.org/abs/2112.08654)]
- `DualPrompt`: DualPrompt: Complementary Prompting for Rehearsal-free Continual Learning. ECCV 2022 [[paper](https://arxiv.org/abs/2204.04799)]
- `CODA-Prompt`: CODA-Prompt: COntinual Decomposed Attention-based Prompting for Rehearsal-Free Continual Learning. CVPR 2023 [[paper](https://arxiv.org/abs/2211.13218)]
- `RanPAC`: RanPAC: Random Projections and Pre-trained Models for Continual Learning. NeurIPS 2023 [[paper](https://arxiv.org/abs/2307.02251)]
- `LAE`: A Unified Continual Learning Framework with General Parameter-Efficient Tuning. ICCV 2023 [[paper](https://arxiv.org/abs/2303.10070)]
- `SLCA`: SLCA: Slow Learner with Classifier Alignment for Continual Learning on a Pre-trained Model. ICCV 2023 [[paper](https://arxiv.org/abs/2303.05118)]
- `FeCAM`: FeCAM:Exploiting the Heterogeneity of Class Distributions in Exemplar-Free Continual Learning. NeurIPS 2023 [[paper](https://proceedings.neurips.cc/paper_files/paper/2023/file/15294ba2dcfb4521274f7aa1c26f4dd4-Paper-Conference.pdf)]
- `DGR`: Gradient Reweighting: Towards Imbalanced Class-Incremental Learning. CVPR 2024 [[paper](https://arxiv.org/abs/2402.18528)]
- `Ease`: Expandable Subspace Ensemble for Pre-Trained Model-Based Class-Incremental Learning. CVPR 2024 [[paper](https://arxiv.org/abs/2403.12030)]
- `CoFiMA`: Weighted Ensemble Models Are Strong Continual Learners. ECCV 2024 [[paper](https://arxiv.org/abs/2312.08977)]
- `SimpleCIL`: Revisiting Class-Incremental Learning with Pre-Trained Models: Generalizability and Adaptivity are All You Need. IJCV 2024 [[paper](https://arxiv.org/abs/2303.07338)]
- `Aper`: Revisiting Class-Incremental Learning with Pre-Trained Models: Generalizability and Adaptivity are All You Need. IJCV 2024 [[paper](https://arxiv.org/abs/2303.07338)]
- `MOS`: Model Surgery for Pre-Trained Model-Based Class-Incremental Learning. AAAI 2025 [[paper](https://arxiv.org/abs/2412.09441)]
- ã€Šå¾®è°ƒï¼ˆFineTuneï¼‰ã€‹ï¼šä¸€ç§åŸºç¡€æ–¹æ³•ï¼Œä»…åœ¨æ–°ä»»åŠ¡ä¸Šæ›´æ–°å‚æ•°ã€‚
- ã€ŠiCaRLã€‹ï¼šå¢é‡åˆ†ç±»å™¨ä¸è¡¨å¾å­¦ä¹ ã€‚å‘è¡¨äº 2017 å¹´è®¡ç®—æœºè§†è§‰ä¸æ¨¡å¼è¯†åˆ«ä¼šè®®ï¼ˆCVPRï¼‰ [[è®ºæ–‡é“¾æ¥](https://arxiv.org/pdf/1611.07725)]
- ã€ŠCoilã€‹ï¼šç”¨äºç±»åˆ«å¢é‡å­¦ä¹ çš„ååŒè¿ç§»æ–¹æ³•ã€‚å‘è¡¨äº 2021 å¹´ç¾å›½è®¡ç®—æœºåä¼šå¤šåª’ä½“ä¼šè®®ï¼ˆACMMMï¼‰ [[è®ºæ–‡é“¾æ¥](https://arxiv.org/pdf/2107.12654)]
- ã€ŠDERã€‹ï¼šç”¨äºç±»åˆ«å¢é‡å­¦ä¹ çš„åŠ¨æ€å¯æ‰©å±•è¡¨å¾ã€‚å‘è¡¨äº 2021 å¹´è®¡ç®—æœºè§†è§‰ä¸æ¨¡å¼è¯†åˆ«ä¼šè®®ï¼ˆCVPRï¼‰ [[è®ºæ–‡é“¾æ¥](https://arxiv.org/pdf/2103.16788)]
- ã€ŠFOSTERã€‹ï¼šç”¨äºç±»åˆ«å¢é‡å­¦ä¹ çš„ç‰¹å¾å¢å¼ºä¸å‹ç¼©æ–¹æ³•ã€‚å‘è¡¨äº 2022 å¹´æ¬§æ´²è®¡ç®—æœºè§†è§‰ä¼šè®®ï¼ˆECCVï¼‰ [[è®ºæ–‡é“¾æ¥](https://arxiv.org/pdf/2204.04662)]
- ã€ŠMEMOã€‹ï¼šä¸€ç§åŸºäº 603 ä¸ªèŒƒä¾‹çš„æ¨¡å‹ï¼šè¿ˆå‘å†…å­˜é«˜æ•ˆçš„ç±»åˆ«å¢é‡å­¦ä¹ ã€‚2023 å¹´å›½é™…å­¦ä¹ è¡¨å¾ä¼šè®®ï¼ˆICLRï¼‰ spotlightï¼ˆäº®ç‚¹ï¼‰è®ºæ–‡ [[è®ºæ–‡é“¾æ¥](https://openreview.net/forum?id=S07feAlQHgM)]
- ã€ŠL2Pã€‹ï¼šä¸ºæŒç»­å­¦ä¹ å­¦ä¹ æç¤ºä¿¡æ¯ã€‚å‘è¡¨äº 2022 å¹´è®¡ç®—æœºè§†è§‰ä¸æ¨¡å¼è¯†åˆ«ä¼šè®®ï¼ˆCVPRï¼‰ [[è®ºæ–‡é“¾æ¥](https://arxiv.org/pdf/2112.08654)]
- ã€ŠDualPromptã€‹ï¼šåŒé‡æç¤ºï¼šç”¨äºæ— æ’ç»ƒæŒç»­å­¦ä¹ çš„äº’è¡¥æç¤ºæ–¹æ³•ã€‚å‘è¡¨äº 2022 å¹´æ¬§æ´²è®¡ç®—æœºè§†è§‰ä¼šè®®ï¼ˆECCVï¼‰ [[è®ºæ–‡é“¾æ¥](https://arxiv.org/pdf/2204.04799)]
- ã€ŠCODA - Promptã€‹ï¼šåŸºäºæŒç»­åˆ†è§£æ³¨æ„åŠ›çš„æç¤ºæ–¹æ³•ï¼Œç”¨äºæ— æ’ç»ƒæŒç»­å­¦ä¹ ã€‚å‘è¡¨äº 2023 å¹´è®¡ç®—æœºè§†è§‰ä¸æ¨¡å¼è¯†åˆ«ä¼šè®®ï¼ˆCVPRï¼‰ [[è®ºæ–‡é“¾æ¥](https://arxiv.org/pdf/2211.13218)]
- ã€ŠRanPACã€‹ï¼šéšæœºæŠ•å½±ä¸é¢„è®­ç»ƒæ¨¡å‹ç”¨äºæŒç»­å­¦ä¹ ã€‚å‘è¡¨äº 2023 å¹´ç¥ç»ä¿¡æ¯å¤„ç†ç³»ç»Ÿå¤§ä¼šï¼ˆNeurIPSï¼‰ [[è®ºæ–‡é“¾æ¥](https://arxiv.org/pdf/2307.02251)]
- ã€ŠLAEã€‹ï¼šä¸€ç§å¸¦æœ‰é€šç”¨å‚æ•°é«˜æ•ˆè°ƒä¼˜çš„ç»Ÿä¸€æŒç»­å­¦ä¹ æ¡†æ¶ã€‚å‘è¡¨äº 2023 å¹´å›½é™…è®¡ç®—æœºè§†è§‰ä¼šè®®ï¼ˆICCVï¼‰ [[è®ºæ–‡é“¾æ¥](https://arxiv.org/pdf/2303.10070)]
- ã€ŠSLCAã€‹ï¼šåŸºäºé¢„è®­ç»ƒæ¨¡å‹è¿›è¡ŒæŒç»­å­¦ä¹ çš„å¸¦åˆ†ç±»å™¨å¯¹é½çš„æ…¢å­¦ä¹ å™¨ã€‚å‘è¡¨äº 2023 å¹´å›½é™…è®¡ç®—æœºè§†è§‰ä¼šè®®ï¼ˆICCVï¼‰ [[è®ºæ–‡é“¾æ¥](https://arxiv.org/pdf/2303.05118)]
- ã€ŠFeCAMã€‹ï¼šåœ¨æ— èŒƒä¾‹æŒç»­å­¦ä¹ ä¸­åˆ©ç”¨ç±»åˆ«åˆ†å¸ƒçš„å¼‚è´¨æ€§ã€‚å‘è¡¨äº 2023 å¹´ç¥ç»ä¿¡æ¯å¤„ç†ç³»ç»Ÿå¤§ä¼šï¼ˆNeurIPSï¼‰ [[è®ºæ–‡é“¾æ¥](https://proceedings.neurips.cc/paper_files/paper/2023/file/15294ba2dcfb4521274f7aa1c26f4dd4-Paper-Conference.pdf)]
- ã€ŠDGRã€‹ï¼šæ¢¯åº¦é‡åŠ æƒï¼šè¿ˆå‘ä¸å¹³è¡¡ç±»åˆ«å¢é‡å­¦ä¹ ã€‚å‘è¡¨äº 2024 å¹´è®¡ç®—æœºè§†è§‰ä¸æ¨¡å¼è¯†åˆ«ä¼šè®®ï¼ˆCVPRï¼‰ [[è®ºæ–‡é“¾æ¥](https://arxiv.org/pdf/2402.18528)]
- ã€ŠEaseã€‹ï¼šç”¨äºåŸºäºé¢„è®­ç»ƒæ¨¡å‹çš„ç±»åˆ«å¢é‡å­¦ä¹ çš„å¯æ‰©å±•å­ç©ºé—´é›†æˆæ–¹æ³•ã€‚å‘è¡¨äº 2024 å¹´è®¡ç®—æœºè§†è§‰ä¸æ¨¡å¼è¯†åˆ«ä¼šè®®ï¼ˆCVPRï¼‰ [[è®ºæ–‡é“¾æ¥](https://arxiv.org/pdf/2403.12030)]
- ã€ŠCoFiMAã€‹ï¼šåŠ æƒé›†æˆæ¨¡å‹æ˜¯å¼ºå¤§çš„æŒç»­å­¦ä¹ å™¨ã€‚å‘è¡¨äº 2024 å¹´æ¬§æ´²è®¡ç®—æœºè§†è§‰ä¼šè®®ï¼ˆECCVï¼‰ [[è®ºæ–‡é“¾æ¥](https://arxiv.org/pdf/2312.08977)]
- ã€ŠSimpleCILã€‹ï¼šé‡æ–°å®¡è§†åŸºäºé¢„è®­ç»ƒæ¨¡å‹çš„ç±»åˆ«å¢é‡å­¦ä¹ ï¼šé€šç”¨æ€§å’Œé€‚åº”æ€§å°±æ˜¯ä½ æ‰€éœ€è¦çš„å…¨éƒ¨ã€‚å‘è¡¨äº 2024 å¹´ã€Šå›½é™…è®¡ç®—æœºè§†è§‰æ‚å¿—ã€‹ï¼ˆIJCVï¼‰ [[è®ºæ–‡é“¾æ¥](https://arxiv.org/pdf/2303.07338)]
- ã€ŠAperã€‹ï¼šé‡æ–°å®¡è§†åŸºäºé¢„è®­ç»ƒæ¨¡å‹çš„ç±»åˆ«å¢é‡å­¦ä¹ ï¼šé€šç”¨æ€§å’Œé€‚åº”æ€§å°±æ˜¯ä½ æ‰€éœ€è¦çš„å…¨éƒ¨ã€‚å‘è¡¨äº 2024 å¹´ã€Šå›½é™…è®¡ç®—æœºè§†è§‰æ‚å¿—ã€‹ï¼ˆIJCVï¼‰ [[è®ºæ–‡é“¾æ¥](https://arxiv.org/pdf/2303.07338)]
- ã€ŠMOSã€‹ï¼šç”¨äºåŸºäºé¢„è®­ç»ƒæ¨¡å‹çš„ç±»åˆ«å¢é‡å­¦ä¹ çš„æ¨¡å‹æ‰‹æœ¯æ–¹æ³•ã€‚å‘è¡¨äº 2025 å¹´ç¾å›½äººå·¥æ™ºèƒ½åä¼šï¼ˆAAAIï¼‰ä¼šè®® [[è®ºæ–‡é“¾æ¥](https://arxiv.org/pdf/2412.09441)]

## ğŸ“ Reproduced Results

#### CIFAR-100

<div align="center">
<img src="./resources/cifarb0inc10.jpg" width="600px">
</div>

#### ImageNet-R

<div align="center">
<img src="./resources/imagenetRb0inc20.jpg" width="600px">
</div>

> For exemplar parameters, Coil, DER, iCaRL, MEMO, and FOSTER set the `fixed_memory` option to false and retain the `memory_size` of 2000 for CIFAR100, while setting `fixed_memory` option to true and retaining the `memory_per_class` of 20 for ImageNet-R. On the contrary, other models are exemplar-free.

## â˜„ï¸ how to use

### ğŸ•¹ï¸ Clone

Clone this GitHub repository:

```
git clone https://github.com/sun-hailong/LAMDA-PILOT
cd LAMDA-PILOT
```

### ğŸ—‚ï¸ Dependencies

1. [torch 2.0.1](https://github.com/pytorch/pytorch)
2. [torchvision 0.15.2](https://github.com/pytorch/vision)
3. [timm 0.6.12](https://github.com/huggingface/pytorch-image-models)
4. [tqdm](https://github.com/tqdm/tqdm)
5. [numpy](https://github.com/numpy/numpy)
6. [scipy](https://github.com/scipy/scipy)
7. [easydict](https://github.com/makinacorpus/easydict)

### ğŸ”‘ Run experiment

1. Edit the `[MODEL NAME].json` file for global settings and hyperparameters.
2. Run:

   ```bash
   python main.py --config=./exps/[MODEL NAME].json
   ```

3. `hyper-parameters`

ä½¿ç”¨PILOTæ—¶ï¼Œä½ å¯ä»¥åœ¨ç›¸åº”çš„JSONæ–‡ä»¶ä¸­ç¼–è¾‘å…¨å±€å‚æ•°ä»¥åŠç‰¹å®šç®—æ³•çš„è¶…å‚æ•°ã€‚

è¿™äº›å‚æ•°åŒ…æ‹¬ï¼š

   - **model_name**: The model's name should be selected from the 11 methods listed above, _i.e._, `finetune`, `icarl`, `coil`, `der`, `foster`, `memo`, `simplecil`, `l2p`, `dualprompt`, `coda-prompt` and `adam`.
   - **init_cls**: The number of classes in the initial incremental stage. As the configuration of CIL includes different settings with varying class numbers at the outset, our framework accommodates diverse options for defining the initial stage.
   - **increment**: The number of classes in each incremental stage $i$, $i$ > 1. By default, the number of classes is equal across all incremental stages.
   - **backbone_type**: The backbone network of the incremental model. It can be selected from a variety of pre-trained models available in the Timm library, such as **ViT-B/16-IN1K** and **ViT-B/16-IN21K**. Both are pre-trained on ImageNet21K, while the former is additionally fine-tuned on ImageNet1K.
   - **seed**: The random seed is utilized for shuffling the class order. It is set to 1993 by default, following the benchmark setting iCaRL.
   - **fixed_memory**: a Boolean parameter. When set to true, the model will maintain a fixed amount of memory per class. Alternatively, when set to false, the model will preserve dynamic memory allocation per class.
   - **memory_size**: The total number of exemplars in the incremental learning process. If `fixed_memory` is set to false, assuming there are $K$ classes at the current stage, the model will preserve $\left[\frac{{memory-size}}{K}\right]$ exemplars for each class. **L2P, DualPrompt, SimpleCIL, ADAM, and CODA-Prompt do not require exemplars.** Therefore, parameters related to the exemplar are not utilized.
   - **memory_per_class**: If `fixed memory` is set to true, the model will preserve a fixed number of `memory_per_class` exemplars for each class.



- **æ¨¡å‹åç§°ï¼ˆmodel_nameï¼‰**ï¼šæ¨¡å‹åç§°åº”ä»ä¸Šè¿°åˆ—å‡ºçš„11ç§æ–¹æ³•ä¸­é€‰å–ï¼Œå³â€œå¾®è°ƒï¼ˆfinetuneï¼‰â€ã€â€œiCaRLâ€ã€â€œCoilâ€ã€â€œDERâ€ã€â€œFOSTERâ€ã€â€œMEMOâ€ã€â€œSimpleCILâ€ã€â€œL2Pâ€ã€â€œDualPromptâ€ã€â€œCODA-Promptâ€ä»¥åŠâ€œADAMâ€ã€‚
- **åˆå§‹ç±»åˆ«æ•°ï¼ˆinit_clsï¼‰**ï¼šåˆå§‹å¢é‡é˜¶æ®µçš„ç±»åˆ«æ•°é‡ã€‚ç”±äºç±»åˆ«å¢é‡å­¦ä¹ ï¼ˆCILï¼‰çš„é…ç½®åœ¨ä¸€å¼€å§‹å°±åŒ…å«äº†ç±»åˆ«æ•°é‡ä¸åŒçš„å„ç§è®¾ç½®ï¼Œæˆ‘ä»¬çš„æ¡†æ¶èƒ½é€‚åº”å®šä¹‰åˆå§‹é˜¶æ®µçš„å¤šç§é€‰é¡¹ã€‚
- **å¢é‡ï¼ˆincrementï¼‰**ï¼šæ¯ä¸ªå¢é‡é˜¶æ®µ\(i\)ï¼ˆ\(i > 1\)ï¼‰çš„ç±»åˆ«æ•°é‡ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œæ‰€æœ‰å¢é‡é˜¶æ®µçš„ç±»åˆ«æ•°é‡æ˜¯ç›¸ç­‰çš„ã€‚
- **éª¨å¹²ç½‘ç»œç±»å‹ï¼ˆbackbone_typeï¼‰**ï¼šå¢é‡æ¨¡å‹çš„éª¨å¹²ç½‘ç»œã€‚å¯ä»¥ä»Timmåº“ä¸­å¯ç”¨çš„å„ç§é¢„è®­ç»ƒæ¨¡å‹ä¸­è¿›è¡Œé€‰æ‹©ï¼Œä¾‹å¦‚â€œè§†è§‰Transformer - B/16 - IN1Kâ€å’Œâ€œè§†è§‰Transformer - B/16 - IN21Kâ€ã€‚è¿™ä¸¤ç§æ¨¡å‹éƒ½åœ¨ImageNet21Kä¸Šè¿›è¡Œäº†é¢„è®­ç»ƒï¼Œè€Œå‰è€…è¿˜åœ¨ImageNet1Kä¸Šè¿›è¡Œäº†é¢å¤–çš„å¾®è°ƒã€‚
- **éšæœºç§å­ï¼ˆseedï¼‰**ï¼šéšæœºç§å­ç”¨äºæ‰“ä¹±ç±»åˆ«é¡ºåºã€‚æŒ‰ç…§iCaRLåŸºå‡†è®¾ç½®ï¼Œé»˜è®¤å€¼ä¸º1993ã€‚
- **å›ºå®šå†…å­˜ï¼ˆfixed_memoryï¼‰**ï¼šä¸€ä¸ªå¸ƒå°”å‹å‚æ•°ã€‚å½“è®¾ç½®ä¸ºâ€œçœŸï¼ˆtrueï¼‰â€æ—¶ï¼Œæ¨¡å‹å°†ä¸ºæ¯ä¸ªç±»åˆ«ç»´æŒå›ºå®šçš„å†…å­˜é‡ã€‚åä¹‹ï¼Œå½“è®¾ç½®ä¸ºâ€œå‡ï¼ˆfalseï¼‰â€æ—¶ï¼Œæ¨¡å‹å°†ä¸ºæ¯ä¸ªç±»åˆ«ä¿ç•™åŠ¨æ€å†…å­˜åˆ†é…ã€‚
- **å†…å­˜å¤§å°ï¼ˆmemory_sizeï¼‰**ï¼šå¢é‡å­¦ä¹ è¿‡ç¨‹ä¸­çš„èŒƒä¾‹æ€»æ•°ã€‚å¦‚æœâ€œå›ºå®šå†…å­˜ï¼ˆfixed_memoryï¼‰â€è®¾ç½®ä¸ºâ€œå‡ï¼ˆfalseï¼‰â€ï¼Œå‡è®¾å½“å‰é˜¶æ®µæœ‰\(K\)ä¸ªç±»åˆ«ï¼Œæ¨¡å‹å°†ä¸ºæ¯ä¸ªç±»åˆ«ä¿ç•™\(\left[\frac{{å†…å­˜å¤§å°ï¼ˆmemory_sizeï¼‰}}{K}\right]\)ä¸ªèŒƒä¾‹ã€‚â€œL2Pâ€ã€â€œDualPromptâ€ã€â€œSimpleCILâ€ã€â€œADAMâ€ä»¥åŠâ€œCODA-Promptâ€ä¸éœ€è¦èŒƒä¾‹ã€‚å› æ­¤ï¼Œä¸èŒƒä¾‹ç›¸å…³çš„å‚æ•°ä¸ä¼šè¢«ä½¿ç”¨ã€‚
- **æ¯ç±»å†…å­˜ï¼ˆmemory_per_classï¼‰**ï¼šå¦‚æœâ€œå›ºå®šå†…å­˜ï¼ˆfixed memoryï¼‰â€è®¾ç½®ä¸ºâ€œçœŸï¼ˆtrueï¼‰â€ï¼Œæ¨¡å‹å°†ä¸ºæ¯ä¸ªç±»åˆ«ä¿ç•™å›ºå®šæ•°é‡ä¸ºâ€œæ¯ç±»å†…å­˜ï¼ˆmemory_per_classï¼‰â€çš„èŒƒä¾‹ã€‚ 





### ğŸ” Datasets

We have implemented the pre-processing datasets as follows:

- **CIFAR100**: will be automatically downloaded by the code.
- **CUB200**: Google Drive: [link](https://drive.google.com/file/d/1XbUpnWpJPnItt5zQ6sHJnsjPncnNLvWb/view?usp=sharing) or Onedrive: [link](https://entuedu-my.sharepoint.com/:u:/g/personal/n2207876b_e_ntu_edu_sg/EVV4pT9VJ9pBrVs2x0lcwd0BlVQCtSrdbLVfhuajMry-lA?e=L6Wjsc)
- **ImageNet-R**: Google Drive: [link](https://drive.google.com/file/d/1SG4TbiL8_DooekztyCVK8mPmfhMo8fkR/view?usp=sharing) or Onedrive: [link](https://entuedu-my.sharepoint.com/:u:/g/personal/n2207876b_e_ntu_edu_sg/EU4jyLL29CtBsZkB6y-JSbgBzWF5YHhBAUz1Qw8qM2954A?e=hlWpNW)
- **ImageNet-A**: Google Drive: [link](https://drive.google.com/file/d/19l52ua_vvTtttgVRziCZJjal0TPE9f2p/view?usp=sharing) or Onedrive: [link](https://entuedu-my.sharepoint.com/:u:/g/personal/n2207876b_e_ntu_edu_sg/ERYi36eg9b1KkfEplgFTW3gBg1otwWwkQPSml0igWBC46A?e=NiTUkL)
- **OmniBenchmark**: Google Drive: [link](https://drive.google.com/file/d/1AbCP3zBMtv_TDXJypOCnOgX8hJmvJm3u/view?usp=sharing) or Onedrive: [link](https://entuedu-my.sharepoint.com/:u:/g/personal/n2207876b_e_ntu_edu_sg/EcoUATKl24JFo3jBMnTV2WcBwkuyBH0TmCAy6Lml1gOHJA?e=eCNcoA)
- **VTAB**: Google Drive: [link](https://drive.google.com/file/d/1xUiwlnx4k0oDhYi26KL5KwrCAya-mvJ_/view?usp=sharing) or Onedrive: [link](https://entuedu-my.sharepoint.com/:u:/g/personal/n2207876b_e_ntu_edu_sg/EQyTP1nOIH5PrfhXtpPgKQ8BlEFW2Erda1t7Kdi3Al-ePw?e=Yt4RnV)
- **ObjectNet**: Onedrive: [link](https://entuedu-my.sharepoint.com/:u:/g/personal/n2207876b_e_ntu_edu_sg/EZFv9uaaO1hBj7Y40KoCvYkBnuUZHnHnjMda6obiDpiIWw?e=4n8Kpy) You can also refer to the [filelist](https://drive.google.com/file/d/147Mta-HcENF6IhZ8dvPnZ93Romcie7T6/view?usp=sharing) if the file is too large to download.

> These subsets are sampled from the original datasets. Please note that I do not have the right to distribute these datasets. If the distribution violates the license, I shall provide the filenames instead.

When training **not** on `CIFAR100`, you should specify the folder of your dataset in `utils/data.py`.

```python
    def download_data(self):
        assert 0,"You should specify the folder of your dataset"
        train_dir = '[DATA-PATH]/train/'
        test_dir = '[DATA-PATH]/val/'
```
